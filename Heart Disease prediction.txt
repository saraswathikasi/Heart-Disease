Heart Disease prediction

This model is used for prediction of a person has heart disease or not it will classify the person having heart disease with 1 or 0

Here we have used the vecstack library which is used for advanced stacking of models(ensemble learning).

We have also imported some more library like pandas - data loading and handling, numpy for numerical operations, matplotlib.pyplot for plotting graphs , scipy for scientific computations

Here Counter is used to Count unique values in data (like how many people have disease vs donâ€™t).

Here the line after splitting the data is using the random_state when it is assigned the splitting of the data is done each time when you run the code.

The word bias is used which means that (the model is giving importance to a feature)

Some features (columns) have very large numerical values, while others have small values, which can unfairly influence the model.

ðŸ“Š Example:
Letâ€™s say you have two features:

Age: ranges from 20 to 80

Cholesterol level: ranges from 100 to 300

Now imagine you feed them into a machine learning model without scaling:

Cholesterol numbers are much larger than age.

The model might "think" cholesterol is more important, just because the numbers are bigger â€” not because it really is.

This introduces bias in learning.

So here we use Standard Scaler which is used for scaling all features in the dataset such that they have  Mean = 0 or Standard deviation = 1 

This removes the bias caused by large or small values.

All features are on a similar scale, so the model treats them equally.

There are multiple models which we have used but the XGBoost is a ensemble method
ðŸ“Œ What is XGBoost?
XGBoost (Extreme Gradient Boosting) is a very powerful and fast machine learning algorithm often used in competitions and real-world projects. It's based on a technique called boosting, where multiple models are trained one after another, and each new model tries to fix the mistakes made by the previous ones.

| Parameter               | Meaning                                                         |
| ----------------------- | --------------------------------------------------------------- |
| `learning_rate=0.01`    | The step size when learning (small = slower but more accurate). |
| `n_estimators=25`       | Total number of trees to build.                                 |
| `max_depth=15`          | Maximum depth of each tree (how detailed it can go).            |
| `gamma=0.6`             | Minimum loss reduction required to make a further split.        |
| `subsample=0.52`        | Fraction of the data used per tree (helps avoid overfitting).   |
| `colsample_bytree=0.6`  | Percentage of features used for each tree.                      |
| `reg_lambda=2`          | L2 regularization (reduces complexity to avoid overfitting).    |
| `booster='dart'`        | Uses dropout (randomly skips trees) like in neural networks.    |
| `colsample_bylevel=0.6` | Fraction of features used at each level of the tree.            |
| `colsample_bynode=0.5`  | Fraction of features used per node split.                       |


Decision Tree Classifier

It's a model that splits data into smaller subsets based on feature values, and then makes a prediction at the end (a "leaf").
It is called a "tree" because:
It starts at a root node (entire dataset).
Splits the dataset using decision rules.
Ends at leaf nodes (final class labels).

1.Root Node	 : The top node representing the whole dataset.
2.Internal Node	 : A feature-based question that splits the dataset.
3.Leaf Node : 	Final prediction (class label).
4. Gini / Entropy	Metrics : to measure how "pure" each split is (entropy = more precise).
Here the criterion taken is entropy which is mainly used to measure the information gain for each split. Helps the tree make more informed, precise splits.
Another option is gini, which is faster and simpler but slightly less accurate in some cases.It can handle both the numerical and categorical data same time.







